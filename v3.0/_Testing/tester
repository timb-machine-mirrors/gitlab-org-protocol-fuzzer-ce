#!/usr/bin/env python

import os
import sys
import argparse
import shutil
import platform
import traceback

sys.path.append(os.path.join('.', '_Testing'))
import dependency
import testlib

if testlib.get_platform() == "win":
    from win32com.shell import shell


LOGDIR = os.environ.get('PEACH_LOGDIR') or \
    os.path.join(os.getcwd(), 'test_logs')
PEACH_RUN_COUNT = os.environ.get('PEACH_RUN_COUNT') or 1
FULL_TARGET_PATH = os.getcwd()
TIMEOUT_MINUTES = os.environ.get('PEACH_TIMEOUT_MINUTES') or 10
ARCH, OS = platform.architecture()
IS_64_BIT = '64' in ARCH
IS_WINDOWS = 'windows' in OS.lower()
IS_INTERACTIVE = sys.stdout.isatty()
USE_COLOR = IS_INTERACTIVE and (not IS_WINDOWS)


################################################################################
# Buildbot stuff is down under
# DETERMINE WHICH PEACH TO FIND


def find_peach():
    if IS_INTERACTIVE:
        return "peach"
    # split the current path by othe os seperator and jump to top of build dir
    _path = os.getcwd().split(os.sep)
    _epeach_idx = _path.index('epeach')
    _path = _path[ : _epeach_idx + 1]
    # piece together the path to the peach.exe we want
    # if it's not windows, assume linux for time being
    # also assuming x86 or x86_64 right now, no ARM
    which_build = ''
    if IS_WINDOWS:
        assert False # oops, notchet. add when you can see the slave build machine
    else:
        which_build += 'linux_x86' #darwin?
    if IS_64_BIT:
        which_build += '_64'
    # debug has better info
    which_build += '_debug'
    _path.extend([which_build, 'build', 'output', which_build, 'bin', 'peach.exe'])
    PEACH_PATH = os.sep.join(_path)
    return PEACH_PATH


###############################################################################


def get_targets(target_path):
    for w in os.walk(target_path):
        w[1].sort()
        curpath = w[0]
        if "_Common" in curpath:
            continue
        file_list = w[2]
        for fn in file_list:
            if fn[-4:].lower() == '.xml':
                yield {"path": curpath, "file": fn}


def user_is_admin():
    if testlib.get_platform() == "win":
        return shell.IsUserAnAdmin()
    else:
        return os.getuid() == 0


###############################################################################
# main
###############################################################################


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description='Test peach pits (this must be run as root)')
    parser.add_argument('-i', '--iterations', dest='iterations',
                        type=int, help='The number of iterations to run',
                        default=PEACH_RUN_COUNT)
    parser.add_argument('-p', '--peach', type=str,
                        help='The location of the peach binary',
                        default=find_peach())
    parser.add_argument('targets', type=str, metavar="target", nargs='*',
                        help=('The the specific targets to test, '
                              'e.g.: Net/TCPv4.xml'))
    parser.add_argument('-l', '--logdir',type=str,
                        help='location to log test output',
                        default=LOGDIR)
    parser.add_argument('-d', '--dependency', action='store_true',
                        required=False, help=('Used to test pit dependencies'))
    parser.add_argument('-c', '--color', type=bool,
                        required=False, default=True,
                        help=('Turns off color codes in output'))
    parser.add_argument('-t', '--timeout', type=int,
                        default=TIMEOUT_MINUTES,required=False, 
                        help=('Time to let pits run before killing them. '
                              'Defaults to 10 minutes.'))
    config = parser.parse_args()
    if not user_is_admin():
        print "You must be root or admin to run this"
        exit(1)
    tests = []
    assert config.timeout >= 0, "timeout cannot be negative"
    if not os.path.isdir(config.logdir):
        os.mkdir(config.logdir)
    test_log = open(os.path.join(config.logdir, "test_status.log"), 'w')
    if config.targets:
        targets = []
        for target in config.targets:
            path = os.path.dirname(target)
            fn = os.path.basename(target)
            if config.dependency:
                pits = dependency.GetPits(path, fn)
                for pit in pits:
                    targets.append(pit)
            targets.append({"path": path, "file": fn})
    else:
        targets = get_targets(FULL_TARGET_PATH)
    for target in targets:
        pit_tests = testlib.get_tests(target, config)
        for test in pit_tests:
            try:
                test.run()
            except Exception:
                traceback.print_exception(*sys.exc_info())
                test.status = "except"
            if test.status == "pass":
                print test.color_text("green", "SUCCESS!")
            elif test.status == "skip":
                print "%s was %s " % (test.pit, test.color_text('yellow', "SKIPPED!"))
            elif test.status == "fail" or test.status == "except":
                print "...and it " + test.color_text("red", "FAILED!")
            elif test.status == "timeout":
                print "...and it " + test.color_text("yellow", "TIMEDOUT!")
            else:
                print test.color_text("red", "UNKNOWN FAILURE!")
            tests.append(test)
    tests.sort(key = lambda obj: obj.name)
    ret = 0
    for test in tests:
        status = test.status
        if test.hasrun:
            status += " after running %s" % test.hasrun
        if test.status in ["pass", "fail"]:
            if test.status == "pass" and IS_INTERACTIVE == False:
                continue #for non interactive we can silently succede. 
            test_log.write(
                "{0:<20} {1:<5} returned {2} for command {3}{4}".format(
                    test.name + " " + status,
                    test.pid,
                    test.returncode,
                    test.cmd,
                    os.linesep))
        ret |= test.returncode #any failure should cause build failure 
        elif test.status == "skip":
            test_log.write("{0:<20}{1}".format(
                    test.name + " was " + status, os.linesep))
        elif test.status == "except":
            test_log.write("{0:<20}{1}".format(
                    test.name + " failed with exception", os.linesep))
    test_log.close()
    return ret

