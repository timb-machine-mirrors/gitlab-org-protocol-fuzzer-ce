:images: ../images
:peachweb: Peach Web Interface
:peachcomd: Peach Command Line Interface
:peachug: Peach User Guide

[[Recipe_LinuxNetClient]]

=== Recipe: Monitoring a Linux Network Service Client

image::{images}/linux_Tux_small.png["Linux Tux mascot", scale="40"]

This recipe describes the base setup needed to fuzz a client of aLinux network service. 
When fuzzing a network client, Peach impersonates the network service, the other endpoint of the network connection.

The recipe is a model that you can follow closely. Or, you can use the model as 
a starting point and augment the model for your specific situation. This recipe 
consists of the following parts: 

1.	The workflow for the fuzzing session
2.	The Peach components to use in configuring the fuzzing setup
This section focuses on the monitoring needs and the agents that house the monitors. 
3.	Configuration settings used to fuzz a service client (snmpwalk) running this workflow

IMPORTANT: Assumptions/Givens in this recipe are that a Pit is ready to use, Peach is ready to run, and any software module needed to perform the fuzzing job is installed.

==== Workflow for the Fuzzing Session

The workflow lists the task sequence that occurs when running the fuzzing session. 
The setup needed to implement the workflow follows in the next section. Start with 
defining the workflow, especially if you plan to embellish the recipe.

Here is the workflow that Peach performs in fuzzing a Linux network service client:

1. Revert to a virtual machine snapshot.
2. Wait for the machine to boot up.
3. Perform fuzzing. Create and run test cases.

.. Peach launches the network client. The client initiates contact with the server and sends requests to the server.
.. Peach impersonates the server and replies to the client queries. Query responses contain fuzzed data.
.. 	Perform fault detection on the client. 
.. If a fault occurs, collect data surrounding the test case.
.. Revert to the VM snapshot.

4.	Loop to step three (resume fuzzing).

==== Peach Components Needed in the Fuzzing Configuration 

Defining the Peach components divides in two parts: identifying the monitors to use in the configuration and identifying where to locate the specified monitors. 

===== Identifying Monitors

This part of the recipe revisits each step of the workflow to identify the monitors needed to implement the configuration:

1. Revert to a snapshot of a virtual machine. 
+
Peach needs to automate the test environment and remove human interaction during the fuzzing job. We place the service in a virtual machine (VM) because Peach can use a VM monitor to automatically start and reset the test environment when needed. 
+
The VM snapshot is taken while the guest OS and the Peach agent are running. Using such a snapshot avoids the wait time associated with booting up the virtual machine. Also, the same snapshot is used when Peach refreshes the test environment after a fault occurs. 
+
The monitor for the VM environment, xref:Monitors_Vmware[VMware] monitor, resides on the host machine.

2. Wait for the machine to boot up.
+
Peach waits for the VM snapshot to resume.

3. Perform fuzzing, checking for faults.

.. Launch the network client. 
+
The Peach agent launches the client service through GDB. The client submits SNMP requests from the remote machine. The debugger and the client both reside on the remote machine.

.. Reply to the client request with fuzzed data.
+
Peach impersonates the network server and sends fuzzed replies in response to client inuireies.

.. Perform fault detection in the VM.
+
The Gdb monitor watches the internals of the services and detects faults such as access violations and exceptions. Again, the debug monitor is located on the same machine as the service.

.. Collect data surrounding each fault as it happens.
+
When a fault occurs, the packets involved with the fault are interesting. Peach captures the packets using a network capture monitor. This monitor resides on the local machine with Peach Fuzzer.
+
Peach collects log files generated by the debugger located on the remote machine where the service resides. A monitor that saves files sends files from a specified folder on the remote system to the Peach logging repository on the local machine. The monitor to save the log files is located on the remote machine.

4. Revert to the VM snapshot.
+
This step uses the VM monitor and VM snapshot from step 1 to refresh the test environment, and the debug monitor from step 3 to start the network service in the refreshed environment. No additional monitors are needed for this step.

5. Resume fuzzing.
+
Loop to step 3. 

===== Identifying Agents

Peach offers two types of agents to manage monitors and I/O publishers: local and remote.

* Local agents reside inside Peach. +
The local agent in this recipe addresses automation involving the VM and data collection 
that captures network packets. The local agent houses the xref:Monitors_Vmware[VMware] 
 and the xref:Monitors_Pcap[NetworkCapture] monitors. 
+
The VMware monitor starts a snapshot VM environment at the beginning of the fuzzing job, 
as well as restarting the same VM snapshot after a fault occurs. 

* Remote agents reside in separate processes on remote machines with the test targets. +
In this case, the remote agent and the Linux service reside on the same machine. 
+
The remote agent houses the xref:Monitors_Gdb[Gdb] debug monitor that starts the 
Linux service client at the beginning of the fuzzing job and restarts the service in the 
refreshed environment after a fault. The Gdb debug monitor detects faults that occur in 
the client. 
+
In addition, a data collection monitor collects log files after a fault occurs in the network client. The xref:Monitors_SaveFile[Save File] monitor forwards the log files to the logging repository.

The result is that we end up with the following configuration:

image::{images}/LinuxNetworkService.png["Configuration for Linux Service with VM", scale="50"]

Peach is located on one machine with a local agent that houses the VM monitor and the Network capture monitor. A second agent resides on the remote machine with the service. The remote agent houses the Gdb debug monitor and the SaveFile monitor. 

The local agent is simple to implement. All that’s needed is to define the agent, then specify the appropriate monitors and monitor settings used with the local agent. 

The remote monitor is a little more involved. Like the local agent, the remote agent needs to be defined, then specify the appropriate monitors and monitor settings used with the remote agent. Second, the remote agent needs to run on the same OS as the test target. This step can be done separately from specifying the configuration details. In this recipe, a VM snapshot is used. See the previous section, Using Virtual Machines, for information on setting up the VM snapshot.

==== Sample configuration Using snmpwalk 

This section shows the recipe implemented for snmpwalk, an SNMP network service client. Using the Peach Web UI, the recipe shows the agents used, the monitors housed in or managed by each agent, and the settings for the Pit variables, the agents, and the monitors.

[NOTE]
=======
The configurations for the network client and the network service are very similar. Two significant differences exist:

* The network client configuration uses a client app, snmpwalk, instead of the network service agent snmpd.
* In the network client configuration, the test target initiates the action instead of
responding to a request. The client contacts Peach, acting as the network service, then waits for Peach to provide a response to the query. The debug monitor has additional configuration options that are set to drive this configuration. 
=======


===== SNMPD Service Setup 

Perform the following task on the VM before taking a snapshot of the VM.

* Run the Peach agent from a shell with root access. +
When Peach starts the VM, the Peach agent is running in a root shell. 

Perform the following item on the local system. 

* Allow access to run the service through the firewall on the local system.


===== Pit Variables 

The following UI display identifies data values needed by the Pit, regardless of the monitors used in the configuration. The screen is modified slightly to focus solely on the Pit-specifc variables.

image::{images}/Recipe_LinuxSvc_Cli_PitVars.png["Pit-specific Variabls for Linux Service Client with VM", scale="50"]

The Pit User Guides describe the Pit-specific variables. In this sample, the SNMP Peach Pit User Guide provides the following descriptions. _Annotations for the variables are italicized_:

SNMP Community String:: Community string used for authentication. The default value is “public”. Peach and the network client must use the same community string.
+
_Check the SNMP server documentation for consistency of this value. If needed, change the value here to coincide with the value expected by the test target._

Source Port:: Port number of the local machine that sends packets to the server. The default value is 162. 
+
_Port 162 is a well-known port value and can be left as is._

Target IPv4 Address:: IPv4 address of the target machine (client). The default value is 127.0.0.1. For information on obtaining the IP v4 address, see Retrieving Machine Information.
+
_Use the IPv4 address reported by ifconfig for one of the interfaces in the VM, such as eth0. For more information, see the Retrieving Machine Information section of the *SNMP Peach Pit User Guide*._

Target Port:: SNMP port number of the remote machine that sends and receives packets. The default value is 161.
+
_Port 161 is a well-known port value and can be left as is._

Timeout:: Duration, in milliseconds, to wait for incoming data. A value of -1 extends the duration to infinity. The default value is 1000 ms. During fuzzing, a timeout failure causes the fuzzer to skip to the next test case.
+
_Use the default value, as it is sufficient for most implementations._

===== Agents 

The following UI diagram acts as an overview, showing the Peach agents and the monitors within each agent. Peach uses the ordering within the agent to determine the order in which to load and run monitors.

image::{images}/Recipe_LinuxSvc_Cli_Agents_n_Monitors.png["Agents and Monitors for Linux Service with VM", scale="50"]

The local agent is defined first and lists the default information for both name and location. This definition for a local agent is typical and, otherwise, unremarkable. The NetworkCapture and Vmware monitors are independent of one another, allowing either monitor to top the list.

The remote agent, named "Remote Client Manager", has quite a different location specification. The location consists of concatenated pieces of information:

* Channel. The channel for a remote agent is `tcp`. A colon and two forward slashes separate the channel from the IP v4 address of the hardware interface. 
* Target IP v4 address. The IP v4 address of the agent is the second component of the location. Use ‘ifconfig’ to find this address of the remote machine.

The monitor list within each agent is significant, as the monitors are launched in order from top to bottom within an agent.

===== Monitors 

This recipe uses four monitors, two on the machine with Peach and two on the remote machine. The recipe shows each monitor and describes its roles (fault detection, data collection, and automation), applicable operating systems, and the most important data fields. 

TIP: The important monitor parameters are identified using the stylized Peach logo adjacent to the entry.

====== Vmware (Remote Client Manager)

The xref:Monitors_Vmware[Vmware] monitor controls setting up and starting the virtual machine and uses the settings in the following illustration:

image::{images}/Recipe_LinuxSvc_Cli_Vmware.png["VMWare Monitor", scale="50"]

The most significant parameters for the VMware monitor follow:

Vmx:: Identifies the full path of the virtual machine image. Peach loads the snapshot of the VM image at the start of the fuzzing job and after a fault occurs.

Headless:: Identifies whether the VM has a window associated with it. When developing a configuration, set this parameter to false. When the configuration is complete, change Headless to true. 

Host Type:: Specifies the VMware product used in the configuration.

Snapshot Name:: Identifies the snapshot to use for the specific image.

===== Network Capture (InterestingPackets)

The xref:Monitors_Pcap[Netowrk Capture Monitor (InterestingPackets)] captures network packets 
sent and received from the test target. When a fault occurs, Peach stores the packets immediately surrounding the fault in the log of the test case.

image::{images}/Recipe_LinuxSvc_Cli_NetCapture.png["Network Capture Monitor", scale="50"]

The most significant parameters for the network capture  monitor follow:

Device:: Specifies the name of the interface on the local machine (the machine 
with Peach) used to communicate with the test target. Use ipconfig to identify 
the interface(s) available for use. 

[NOTE]
=======
You can find the appropriate host interface that communicates with the VM using the following steps:

1. Collect a list of interfaces (and their IPv4 addresses) by running ipconfig or ifconfig.
2. Test each interface in the list. Manually run a capture session with Wireshark using an interface from the list. 
3. On the host machine, Ping the target IPv4 (of the VM).
4. If the correct interface of the host is used, you’ll see the Ping request and reply packet exchanges through Wireshark,
5. Loop to step 2 and repeat, using another interface. 

=======

Filter:: Helps capture only those packets associated with the fuzzing session. The filter adheres to the syntax and requirements of the Pcap filter specification.

TIP: WireShark refers to the Libpcap filters as capture filters. Use the capture filters.
Wireshark also defines its own display filters that it uses to filter entries in its 
session files. The display filters are not compatible with Libpcap.

===== Gdb (Debugger)

The xref:Monitors_Gdb[Gdb] debugger monitor performs two main functions in this recipe:

* Starts the network client at the start of a fuzzing job and restarts the client when the VM snapshot refreshes.
* Detects faults internal to the client.

The Gdb monitor uses the settings in the following illustration:

image::{images}/Recipe_LinuxSvc_Cli_Gdb.png["Gdb Monitor", scale="50"]

The most significant paramters follow:

Executable name:: Identifies the full path to the SNMP Linux client, snmpwalk. The client resides on the remote matchine; so, the full path is for the Linux file system.

Arguments:: Arguments for the executable. Here, the snmpwalk arguments consist of the authentication version, the community string, and the destination prot of the client queries.

No Cpu Kill:: Controls whether the process stays alive if its CPU usage drops to zero. Specify `true` to keep the process running and to allow the process to release or close its resources before exiting. For more information, see section XXX.

Start On Call:: Controls when the test target launches, and in turn, initiates contact with the service (Peach). Specify `StartIterationEvent` to launch the client at the start of the test case. 

===== Savefile (CollectLogs) 

The xref:Monitors_SaveFile[SaveFile] monitor collects log files from the remote test target and 
copies them ito the Peach Logging folder. The monitor is housed by the remote agent.

image::{images}/SaveFileMonitor2.png["SaveFile Monitor", scale="50"]

The most significant paramter follows:

Filename:: Specifies the full path to the Linux logging system used by GDB. 

