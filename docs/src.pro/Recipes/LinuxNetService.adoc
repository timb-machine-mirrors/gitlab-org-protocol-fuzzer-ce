:images: ../images
:peachweb: Peach Web Interface
:peachcomd: Peach Command Line Interface
:peachug: Peach User Guide

[[Recipe_LinuxNetServer]]

=== Recipe: Monitoring a Linux Network Service

image::{images}/linux_Tux_small.png["Linux Tux mascot", scale="40"]

This recipe describes the base setup needed to fuzz a Linux network service. 
When fuzzing a network server, Peach impersonates the client, the other endpoint of the 
network connection.

The recipe is a model that you can follow closely. Or, you can use the model as 
a starting point and augment the model for your specific situation. This recipe 
consists of the following parts: 

1.	The workflow for the fuzzing session
2.	The Peach components to use in configuring the fuzzing setup
This section focuses on the monitoring needs and the agents that house the monitors. 
3.	Configuration settings used to fuzz a sample service (SNMP) running this workflow

IMPORTANT: Assumptions/Givens in this recipe are that a Pit is ready to use, Peach is ready to run, and any software module needed to perform the fuzzing job is installed.

In this scenario, Peach runs on a host computer; the network server runs in a 
Virtual Machine (VM) on the host. With Peach running on the host, it controls the 
environment. If the netowrk server crashes, the worst thing that happens is that 
the virtual machine has to restart. Peach recover the data if the network 
server crashes. 

==== Workflow for the Fuzzing Session

The workflow lists the task sequence that occurs when running the fuzzing session. 
The setup needed to implement the workflow follows in the next section. Start with 
defining the workflow, especially if you plan to embellish the recipe.

Here is the workflow that Peach performs in fuzzing a Linux network service:

1. Revert to a virtual machine snapshot.
2. Wait for the machine to boot up.
3. Launch the network service.
4. Perform fuzzing. Create and run test cases.

* Peach initiates contact with the server and sends packets of fuzzed data to the server.
* Check for faults, such as crashes and access violations.

5. When a fault occurs, do the following:

..	Collect data surrounding the test case.
..	Revert to the VM snapshot.
..	Launch the network service.

6.	Loop to step 4 (resume fuzzing).


==== Peach Components Needed in the Fuzzing Configuration 

Defining the Peach components divides in two parts: identifying the monitors to use in the configuration and identifying where to locate the specified monitors. 

===== Identifying Monitors

This part of the recipe revisits each step of the workflow to identify the monitors needed to implement the configuration:

.Step 1. Revert to a snapshot of a virtual machine. 

Peach needs to automate the test environment and remove human interaction during the fuzzing job. We place the service in a virtual machine (VM) because Peach can use a VM monitor to automatically start and reset the test environment when needed. 

The VM snapshot is taken while the guest OS and the Peach agent are running. Using such a snapshot avoids the wait time associated with booting up the virtual machine. Also, the same snapshot is used when Peach refreshes the test environment after a fault occurs. 

The monitor for the VM environment, xref:Monitors_Vmware[VMware] monitor, resides on the host machine.

.Step 2. Wait for the machine to boot up.

Peach waits for the VM snapshot to resume.

.Step 3. Launch the network service. 

The Peach agent in the VM starts the network service via the xref:Monitors_Gdb[Gdb] debugging monitor and the GDB system debugger.

.Step 4. Perform fuzzing, checking for faults.

Perform fault detection in the VM. The Gdb monitor watches the internals of the services and detects faults such as access violations and exceptions. Again, the debug monitor is located on the same machine as the service.

.Step 5. Collect data surrounding each fault as it happens.

Peach sends and receives network packets to the service. When a fault occurs, the packets involved with the fault are interesting. Peach captures the packets using the xref:Monitors_Pcap[NetworkCapture] monitor. 

Peach collects log files generated by the debugger located on the remote machine where the service resides. A xref:Monitors_SaveFile[Save File] monitor sends files from a specified folder on the remote system to the Peach logging repository on the local machine.

.Step 6. Resume fuzzing.

This step uses the VM monitor and VM snapshot from step 1 to refresh the test environment, and the debug monitor from step 3 to start the network service in the refreshed environment. No additional monitors are needed for this step. 

===== Identifying Agents

Peach offers two types of agents to manage monitors and I/O publishers: local and remote.

* Local agents reside inside Peach. +
The local agent in this recipe addresses automation involving the VM and data collection 
that captures network packets. The local agent houses the xref:Monitors_Vmware[VMware] 
 and the xref:Monitors_Pcap[NetworkCapture] monitors. 
+
The VMware monitor starts a snapshot VM environment at the beginning of the fuzzing job, 
as well as restarting the same VM snapshot after a fault occurs. 

* Remote agents reside in separate processes on remote machines with the test targets. +
In this case, the remote agent and the Linux service reside on the same machine. 
+
The remote agent houses the xref:Monitors_Gdb[Gdb] debug monitor that starts the 
network service at the beginning of the fuzzing job and restarts the service in the 
refreshed environment after a fault. The Gdb debug monitor detects faults that occur in 
the service. 
+
In addition, a data collection monitor collects log files after a fault occurs in the network service. The xref:Monitors_SaveFile[Save File] monitor forwards the log files to the logging repository.

The result is that we end up with the following configuration:

image::{images}/LinuxNetworkService.png["Configuration for Linux Service with VM", scale="50"]

Peach is located on one machine with a local agent that houses the VM monitor and the Network capture monitor. A second agent resides on the remote machine with the service. The remote agent houses the Gdb debug monitor and the SaveFile monitor. 

The local agent is simple to implement. All that’s needed is to define the agent, then specify the appropriate monitors and monitor settings used with the local agent. 

The remote monitor is a little more involved. Like the local agent, the remote agent needs to be defined, then specify the appropriate monitors and monitor settings used with the remote agent. Second, the remote agent needs to run on the same OS as the test target. This step can be done separately from specifying the configuration details. In this recipe, a VM snapshot is used. See the previous section, Using Virtual Machines, for information on setting up the VM snapshot.

==== Sample configuration Using SNMP 

This section shows the recipe implemented for the SNMP network service and consists of the following items:

* Settings for the SNMPD service on the Linux VM 
* Pit variables 
* Peach agents
* Peach monitors

===== SNMPD Service Setup 

Perform the following items on the VM before taking a snapshot of the VM.

1.	Run the Peach agent from a shell with root access. +
When Peach starts the VM, the Peach agent is running in a root shell. 
2.	Have SNMP listen for connections on all IPv4 interfaces.
  a. In the VM, edit the snmpd configuration file /etc/snmp/snmpd.conf. + 
    Change the following line: agentAddress  udp:127.0.0.1:161 +
    to the following:          agentAddress  udp:161
  b. Save the change.
3.	Stop the SNMP service. +
During fuzzing, GDB will start the service. 

The following action is performed on the local system. 

* Allow access to run the service through the firewall on the local system.


===== Pit Variables 

The following UI display identifies data values needed by the Pit, regardless of the monitors used in the configuration. The screen is modified slightly to focus solely on the Pit-specifc variables.

image::{images}/NetSvcRecipe_PitVars.png["Pit-specific Variabls for Linux Service with VM", scale="50"]

The Pit User Guides describe the Pit-specific variables. In this sample, the SNMP Peach Pit User Guide provides the following descriptions. _Annotations for the variables are italicized_:

SNMP Community String:: Community string used for authentication by the SNMP server. The default value is “public”. The target SNMP server must be configured to respond to this community string.
+
_Check the SNMP server documentation for consistency of this value. If needed, change the value here to coincide with the value expected by the test target._

Source Port:: Port number of the local machine that sends packets to the server. The default value is 162. 
+
_Port 162 is a well-known port value and can be left as is._

Target IPv4 Address:: IPv4 address of the target machine (server). The default value is 127.0.0.1. For information on obtaining the IP v4 address, see Retrieving Machine Information.
+
_Use the IPv4 address reported by ifconfig for one of the interfaces in the VM, such as eth0. For more information, see the Retrieving Machine Information section of the *SNMP Peach Pit User Guide*._

Target Port:: SNMP port number of the server that receives packets. The default value is 161.
+
_Port 161 is a well-known port value and can be left as is._

Timeout:: Duration, in milliseconds, to wait for incoming data. A value of -1 extends the duration to infinity. The default value is 1000 ms. During fuzzing, a timeout failure causes the fuzzer to skip to the next test case.
+
_Use the default value, as it is sufficient for most implementations._

===== Agents 

The following UI diagram acts as an overview, showing the Peach agents and the monitors within each agent. Peach uses the ordering within the agent to determine the order in which to load and run monitors.

image::{images}/NetSvcRecipe_Agents.png["Agents and Monitors for Linux Service with VM", scale="50"]

The local agent is defined first and lists the default information for both name and location. This definition for a local agent is typical and, otherwise, unremarkable. The Vmware monitor, that starts the virtual machine, is the first monitor listed, as that action is not dependent on actions from another monitor. 

The remote agent, named "Remote", has quite a different location specification. The location consists of concatenated pieces of information:

* Channel. The channel for a remote agent is `tcp`. A colon and two forward slashes separate the channel from the IP v4 address of the hardware interface. 
* IP v4 address. The IP v4 address of the agent is the second component of the location. Use ‘ifconfig’ to  find this address of the remote machine.

The monitor list within each agent is significant, as the monitors are launched in order from top to bottom within an agent.

===== Monitors 

This recipe uses four monitors, two on the machine with Peach and two on the remote machine. The recipe shows each monitor and describes its roles (fault detection, data collection, and automation), applicable operating systems, and the most important data fields. 

TIP: The important monitor parameters are identified using the stylized Peach logo adjacent to the entry.

====== Vmware (Linux virtual machine Automation)

The xref:Monitors_Vmware[Vmware] monitor controls setting up and starting the virtual machine and uses the settings in the following illustration:

image::{images}/Vmware_Monitor2.png["VMWare Monitor", scale="50"]

The most significant parameters for the VMware monitor follow:

Vmx:: Identifies the full path of the virtual machine image. Peach loads the snapshot of the VM image at the start of the fuzzing job and after a fault occurs.

Headless:: Identifies whether the VM has a window associated with it. When developing a configuration, set this parameter to false. When the configuration is complete, change Headless to true. 

Host Type:: Specifies the VMware product used in the configuration.

Snapshot Name:: Identifies the snapshot to use for the specific image.

===== Network Capture (InterestingPackets)

The xref:Monitors_Pcap[Netowrk Capture Monitor (InterestingPackets)] captures network packets 
sent and received from the test target. When a fault occurs, Peach stores the packets immediately surrounding the fault in the log of the test case.

image::{images}/NetworkCapture_PCap2.png["Network Capture Monitor", scale="50"]

The most significant parameters for the network capture  monitor follow:

Device:: Specifies the name of the interface on the local machine (the machine with Peach) 
used to communicate with the test target. Use ifconfig to identify the interface(s) 
available for use. 

[NOTE]
=======
You can find the appropriate host interface that communicates with the VM using the following steps:

1. Collect a list of interfaces (and their IPv4 addresses) by running ipconfig or ifconfig.
2. Test each interface in the list. Manually run a capture session with Wireshark using an interface from the list. 
3. On the host machine, Ping the target IPv4 (of the VM).
4. If the correct interface of the host is used, you’ll see the Ping request and reply packet exchanges through Wireshark,
5. Loop to step 2 and repeat, using another interface. 

=======

Filter:: Helps capture only those packets associated with the fuzzing session. The filter adheres to the syntax and requirements of the Pcap filter specification.

TIP: WireShark refers to the Libpcap filters as capture filters. Use the capture filters.
Wireshark also defines its own display filters that it uses to filter entries in its 
session files. The display filters are not compatible with Libpcap.

===== Gdb (Debugger)

The xref:Monitors_Gdb[Gdb] debugger monitor performs two main functions in this recipe:

* Starts the network service at the start of a fuzzing job and restarts the service when the VM snapshot refreshes.
* Detects faults internal to the service.

The Gdb monitor uses the settings in the following illustration:

image::{images}/Gdb_Monitor_VM2.png["Gdb Monitor", scale="50"]

The most significant paramters follow:

Executable name:: Identifies the full path to the SNMP Linux service. This monitor, managed by the remote agent, resides on the remote machine, so the full path is for the Linux file system. 

Arguments:: Arguments for the executable. Here, the `-f` argument is used so that snmpd runs in the foreground.

===== Savefile (CollectLogs) 

The xref:Monitors_SaveFile[SaveFile] monitor collects log files from the remote test target and 
copies them ito the Peach Logging folder. The monitor is housed by the remote agent.

image::{images}/SaveFileMonitor2.png["SaveFile Monitor", scale="50"]

The most significant paramter follows:

Filename:: Specifies the full path to the Linux logging system used by GDB. 

